# -*- coding: utf-8 -*-
"""SketchAI_PWAIC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XyK0OCfP9cJjjCnObdsETZSzOGR9lOSb

# I. Preparing the Data

### Imports
"""

import os
import glob
import numpy as np
from tensorflow.keras import layers
from tensorflow import keras
import tensorflow as tf
import urllib.request

"""### Getting the list of classes"""

os.system("""wget 'https://raw.githubusercontent.com/pwaic/pwaic.github.io/master/projects/SketchAI/sketchAI_classList.txt'""")

all_classes = []

with open('sketchAI_classList.txt', 'r') as f:
    all_classes = f.readlines()

all_classes = [x.strip('\r\n\t ') for x in all_classes]
print(all_classes)

"""### Download data off Google servers"""

if not os.path.isdir('data'):
    os.mkdir('data')

    base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'
    for c in all_classes:
        cls_url = c.replace('_', '%20')
        path = base + cls_url + '.npy'
        print(path)
        urllib.request.urlretrieve(path, 'data/' + c + '.npy')

"""### Load data"""

def load_data(root, vfold_ratio=0.2, max_items_per_class= 100000):
    all_files = glob.glob(os.path.join(root, '*.npy'))

    #initialize variables 
    x = np.empty([0, 784])
    y = np.empty([0])
    class_names = []

    #load each data file 
    for idx, file in enumerate(all_files):
        data = np.load(file)
        data = data[0: max_items_per_class, :]
        labels = np.full(data.shape[0], idx)

        x = np.concatenate((x, data), axis=0)
        y = np.append(y, labels)

        class_name, ext = os.path.splitext(os.path.basename(file))
        class_names.append(class_name)

    data = None
    labels = None
    
    #randomize the dataset 
    permutation = np.random.permutation(y.shape[0])
    x = x[permutation, :]
    y = y[permutation]

    #separate into training and testing 
    vfold_size = int(x.shape[0] / 100 * (vfold_ratio * 100))

    x_test = x[0:vfold_size, :]
    y_test = y[0:vfold_size]

    x_train = x[vfold_size:x.shape[0], :]
    y_train = y[vfold_size:y.shape[0]]
    return x_train, y_train, x_test, y_test, class_names

x_train, y_train, x_test, y_test, class_names = load_data('data')
num_classes = len(class_names)
image_size = 28
print(len(x_train))

"""### Show some random data"""

import matplotlib.pyplot as plt
from random import randint
# %matplotlib inline  
idx = randint(0, len(x_train))
plt.imshow(x_train[idx].reshape(28,28)) 
print(class_names[int(y_train[idx].item())])

"""### Data preprocessing"""

# Reshape and normalize
x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')
x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')

x_train /= 255.0
x_test /= 255.0

# Convert class vectors to class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

y_train = np.argmax(y_train, 1)
y_test = np.argmax(y_test, 1)

print('x_train size: {}\nx_test size: {}\ny_train size: {}\ny_test size:{}'.format(
    len(x_train), len(x_test), len(y_train), len(y_test)))

"""# II. Program the Model"""

# Define model
model = keras.Sequential()
model.add(layers.Convolution2D(16, (3, 3),
                               padding='same',
                               input_shape=x_train.shape[1:], activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
model.add(layers.Convolution2D(32, (3, 3), padding='same', activation= 'relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
model.add(layers.Convolution2D(64, (3, 3), padding='same', activation= 'relu'))
model.add(layers.MaxPooling2D(pool_size =(2,2)))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(100, activation='softmax')) 

# Train model
model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])

print(model.summary())

"""# III. Train the Model"""

model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 256, verbose=2, epochs=10)

"""# IV. Test the Model

### Evaluate overall accuracy
"""

score = model.evaluate(x_test, y_test, verbose=0)
print('Test accuracy: {:0.2f}%'.format(score[1] * 100))

"""### Test on random dataset element"""

idx = randint(0, len(x_test))
img = x_test[idx]
plt.imshow(img.squeeze()) 
pred = model.predict(np.expand_dims(img, axis=0))[0]
ind = (-pred).argsort()[:5]
latex = [class_names[x] for x in ind]
print(latex)

"""# V. Deploy the Model

### Install TensorFlow.js
"""

os.system("pip install tensorflowjs==0.8.0")

"""### Save and convert model"""

model.save('keras.h5')
os.system("mkdir model")
os.system("tensorflowjs_converter --input_format keras keras.h5 model/")

"""### Zip and download model"""

os.system("cp sketchAI_classList.txt model/class_names.txt")

os.system("zip -r model.zip model")

from google.colab import files
files.download('model.zip')